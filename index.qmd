---
title: "Webscraping with RSelenium"
subtitle: "Automate your browser actions"
author: "Etienne Bacher"
date: "March 17, 2023"
institute: "LISER"

format: 
 revealjs:
   incremental: false  
   theme: [moon, custom.scss]
   pdf-separate-fragments: true
   strip-comments: true
   highlight-style: atom-one
   auto-animate-duration: 0.8
   code-copy: true
   slide-number: true
   filters: [custom.lua]
   
execute:
  eval: false
  echo: true
---

<!-- Print to PDF -->
<!-- Follow this: https://quarto.org/docs/presentations/revealjs/presenting.html#print-to-pdf -->
<!-- Use chrome and not firefox -->


<br><br>

* Motivation: when do we need RSelenium?

<br>

* Get started

<br>

* Basic example

<br>

* Advanced example


# Static and dynamic pages 

## Static and dynamic pages

<br><br>

The web works with 3 languages:

-   HTML: content and structure of the page
-   CSS: style of the page
-   JavaScript: interactions with the page


## Static and dynamic pages

<br><br>

The web works with 3 languages:

-   HTML: content and structure of the page

-   CSS: style of the page

-   [**JavaScript**: interactions with the page]{style="color: beige;"}


## Static vs dynamic

<br><br>

**Static webpage**:

-   all the information is loaded with the page;
-   changing a parameter modifies the URL

<br>

Examples: [Wikipedia](https://en.wikipedia.org/wiki/Web_scraping){.external target="_blank"}, [IMDB](https://www.imdb.com/name/nm0001392/?ref_=nv_sr_srsg_0){.external target="_blank"}, elections results from [El Pais](https://resultados.elpais.com/elecciones/2019/municipales/), etc.

---

Example: elections results in Spain from the website of [El Pais](https://resultados.elpais.com/elecciones/2019/municipales/){.external target="_blank"}

![](img/elpais1.png){width="60%"}

![](img/elpais2.png){width="60%"}

<br>

## Static vs dynamic

<br><br>

**Dynamic webpage**: the website uses JavaScript to fetch data from
their server and *dynamically* update the page.

<br>

Example: [Premier League stats](https://www.premierleague.com/stats/top/players/goal_assist){.external target="_blank"}.

---

Example: [Premier League stats](https://www.premierleague.com/stats/top/players/goal_assist){.external target="_blank"}

![](img/premier-league-1.png){width="60%"}

![](img/premier-league-2.png){width="60%"}



# Why is it harder to do webscraping with dynamic pages?

---

<br>

Webscraping a static website can be quite simple:

-   you get a list of URLs;
-   download the HTML for each of them;
-   read and clean the HTML

and that's it.

<br>

. . .

This is "easy" because you can identify two pages with different content just by
looking at their URL.



---

<br> <br> <br> <br> 

Static webscraping can be challenging: need to have good error handling, the HTML 
itself can be hard to clean, etc.

. . .

<br> 

But in dynamic pages, there's no obvious way to see that the inputs are
different, so it seems that the only way to get the data is to go manually through
all pages.



# (R)Selenium

## Idea

Idea: control the browser from the command line.

<br>

. . .

*"I wish I could click on this button to open a modal"*

```{r}
remote_driver$
  findElement(using = "css", value = ".my-button")$
  clickElement()
```

<br>

. . .

*"I wish I could fill these inputs to automatically connect"*

```{r}
remote_driver$
  findElement(using = "id", value = "password")$
  sendKeysToElement(list("my_super_secret_password"))
```

---

<br> 

Almost everything you can do "by hand" in a browser, you can reproduce
with Selenium:

. . .

| Action                                       | Code                       |
|-------------------|----------|
| Open a browser                                    | `open()` / `navigate()`    |
| Click on something                                | `clickElement()`           |
| Enter values                                      | `sendKeysToElement()`      |
| Go to previous/next page                          | `goBack()` / `goForward()` |
| Refresh the page                                  | `refresh()`                |
| Get all the HTML that is currently <br> displayed | `getPageSource()`          |



# Get started

## Get started

<br> 

To initialize RSelenium, use `rsDriver()`:

```{r}
# if not already installed
# install.packages("RSelenium")
library(RSelenium)

driver <- rsDriver(browser = "firefox", chromever = NULL) # can also be "chrome"
remote_driver <- driver[["client"]]
```

 <!-- chromever = NULL necessary to avoid recent error: 
 see: https://stackoverflow.com/a/74735571 -->

. . .

<br> 

If everything works fine, this will print a bunch of messages and open a "marionette browser".

![](img/marionette2.png)



## Get started

From now on, the main thing is to call `<function>()` starting with
`remote_driver$`[^1].

[^1]: Or whatever you called it in the previous step.

<img src="img/rsdriver_funcs.png" alt="drawing" style="width:75%; text-align: center"/>



# Basic example

## Basic example

<br> 

**Objective:** get the list of core contributors to R located
[on the R-project website](https://www.r-project.org/contributors.html){.external target="_blank"}.

. . .

<br> 

How would you do it by hand?

-   open the browser;
-   go to [https://r-project.org](https://r-project.org);
-   in the left sidebar, click on the link "Contributors";

and voilà!

. . .

<br>

How can we do these steps programmatically?


## Open the browser and navigate

```{r}
remote_driver$navigate("https://r-project.org")
```

<img src="img/rproject.png" alt="drawing" style="width:75% !important; text-align: center !important"/>



## Click on "Contributors"

<br>

This requires two things:

1.  find the element
2.  click on it

---

<br>

1. Find the element

![](img/console_2.png)

---

<br><br>

How can we find this link with `RSelenium`?

```{r}
?RSelenium::remoteDriver()
```


<br> 

-> `findElement`

. . .

::: columns
::: {.column width="45%"}
-   class name ❌
-   id ❌
-   name ❌
-   tag name ❌
:::

::: {.column width="10%"}
:::

::: {.column width="45%"}
-   css selector ✔️
-   link text ✔️
-   partial link text ✔️
-   xpath ✔️
:::
:::


---

<br>

2. Click it:

```{r}
remote_driver$
  findElement("link text", "Contributors")$
  clickElement()
```

---

We are now on the right page!

![](img/contributors.png)

---

<br><br>

Last step: obtain the HTML of the page.

```{r}
remote_driver$getPageSource()
```

<br>

. . .

To read it with the package `rvest`:

```{r}
x <- remote_driver$getPageSource()[[1]]
rvest::read_html(x)
```


<br>

Click [here](#contributors-results) to see the results.


# Advanced example

---

<br><br>


The previous example was not a *dynamic* page: we could have used the
link to the page and apply webscraping methods for static webpages.

<br>

```{r}
rvest::read_html("https://www.r-project.org/contributors.html")
```

. . .

<br>

Let's now dive into a more complex example, where RSelenium is the only
way to obtain the data.


## Example: Sao Paulo immigration museum

<br>

[Open website](http://www.inci.org.br/acervodigital/livros.php){.external target="_blank"}

<br>

Steps:

1. Open the website
1. Enter "PORTUGUESA" in the input box
1. Wait a bit for the page to load
1. Open every modal "Ver Mais"



## Quick demo {auto-animate=true}

Initialize the remote driver and go to the website:

```{r}
library(RSelenium)

link <- "http://www.inci.org.br/acervodigital/livros.php"

# Automatically go the website
driver <- rsDriver(browser = c("firefox"), chromever = NULL)
remote_driver <- driver[["client"]]
remote_driver$navigate(link)
```


## Quick demo {auto-animate=true}

Fill the field "NACIONALIDADE":

```{.r code-line-numbers="10-13"}
library(RSelenium)

link <- "http://www.inci.org.br/acervodigital/livros.php"

# Automatically go the website
driver <- rsDriver(browser = c("firefox"), chromever = NULL)
remote_driver <- driver[["client"]]
remote_driver$navigate(link)

# Fill the nationality field and click on "Validate"
remote_driver$
  findElement(using = "id", value = "nacionalidade")$
  sendKeysToElement(list("PORTUGUESA"))
```

## Quick demo {auto-animate=true}

Find the button "Pesquisar" and click it:

```{.r code-line-numbers="15-18"}
library(RSelenium)

link <- "http://www.inci.org.br/acervodigital/livros.php"

# Automatically go the website
driver <- rsDriver(browser = c("firefox"), chromever = NULL)
remote_driver <- driver[["client"]]
remote_driver$navigate(link)

# Fill the nationality field and click on "Validate"
remote_driver$
  findElement(using = "id", value = "nacionalidade")$
  sendKeysToElement(list("PORTUGUESA"))

# Find the button "Pesquisar" and click it
remote_driver$
  findElement(using = 'name', value = "Reset2")$
  clickElement()
```


## Quick demo {auto-animate=true}

Find the button "Ver Mais" and click it:

```{.r code-line-numbers="20-23"}
library(RSelenium)

link <- "http://www.inci.org.br/acervodigital/livros.php"

# Automatically go the website
driver <- rsDriver(browser = c("firefox"), chromever = NULL)
remote_driver <- driver[["client"]]
remote_driver$navigate(link)

# Fill the nationality field and click on "Validate"
remote_driver$
  findElement(using = "id", value = "nacionalidade")$
  sendKeysToElement(list("PORTUGUESA"))

# Find the button "Pesquisar" and click it
remote_driver$
  findElement(using = 'name', value = "Reset2")$
  clickElement()

# Find the button "Ver Mais" and click it
remote_driver$
  findElement(using = 'id', value = "link_ver_detalhe")$
  clickElement()
```


## Quick demo {auto-animate=true}

Get the HTML that is displayed:

```{.r code-line-numbers="25-26"}
library(RSelenium)

link <- "http://www.inci.org.br/acervodigital/livros.php"

# Automatically go the website
driver <- rsDriver(browser = c("firefox"), chromever = NULL)
remote_driver <- driver[["client"]]
remote_driver$navigate(link)

# Fill the nationality field and click on "Validate"
remote_driver$
  findElement(using = "id", value = "nacionalidade")$
  sendKeysToElement(list("PORTUGUESA"))

# Find the button "Pesquisar" and click it
remote_driver$
  findElement(using = 'name', value = "Reset2")$
  clickElement()

# Find the button "Ver Mais" and click it
remote_driver$
  findElement(using = 'id', value = "link_ver_detalhe")$
  clickElement()

# Get the HTML that is displayed in the modal
x <- remote_driver$getPageSource()
```


## Quick demo {auto-animate=true}

Exit the modal by pressing "Escape":

```{.r code-line-numbers="28-31"}
library(RSelenium)

link <- "http://www.inci.org.br/acervodigital/livros.php"

# Automatically go the website
driver <- rsDriver(browser = c("firefox"), chromever = NULL)
remote_driver <- driver[["client"]]
remote_driver$navigate(link)

# Fill the nationality field and click on "Validate"
remote_driver$
  findElement(using = "id", value = "nacionalidade")$
  sendKeysToElement(list("PORTUGUESA"))

# Find the button "Pesquisar" and click it
remote_driver$
  findElement(using = 'name', value = "Reset2")$
  clickElement()

# Find the button "Ver Mais" and click it
remote_driver$
  findElement(using = 'id', value = "link_ver_detalhe")$
  clickElement()

# Get the HTML that is displayed in the modal
x <- remote_driver$getPageSource()

# Exit the modal by pressing "Escape"
remote_driver$
  findElement(using = "xpath", value = "/html/body")$
  sendKeysToElement(list(key = "escape"))
```


## Next steps

<br><br>

1. Do this for all modals

<br>

2. Once all modals are scraped, go to the next page

<br>

3. Hope that your code runs smoothly for 2000 pages.



## And then what?


<br> <br>

If everything goes well, we have collected a lot of `.html` files.

<br>

To clean them, we don't need `RSelenium` or an internet connection. These are just
text files, they are not "tied" to the website anymore.



# Summary

---

<br>

1. `Selenium` in general is a very useful tool but should be used as a last resort:

  * APIs, packages
  * static webscraping is usually faster
  * custom `POST` requests

. . .

2. Extensions:

  * parallelize `RSelenium` with [`parsel`](https://github.com/till-tietz/parsel){.external target="_blank"} (**never tried**)
  * there's [some WIP](https://github.com/tidyverse/rvest/pull/362){.external target="_blank"} in `rvest` to enable this kind of dynamic scraping


## Ethics


Pay attention to a website's Terms of Use/Service.

<br>

Some websites explicitely say that you are not allowed to programmatically access
their resources.

<br>

![](img/terms-of-use.png)


## Ethics

<br> <br>

**Be respectful**: make the scraping slow enough not to overload the server.

<br>

Not every website can handle tens of thousands of requests very quickly.

<br>

::: {.callout-tip}
For static webscraping, check out the package [`polite`](https://dmi3kno.github.io/polite/){.external target="_blank"}.
:::



## Good resources

<br>

Article from Appsilon: 

[https://appsilon.com/webscraping-dynamic-websites-with-r/](https://appsilon.com/webscraping-dynamic-websites-with-r/){.external target="_blank"}

<br>

Article from Ivan Millanes:

[https://ivanmillanes.netlify.app/post/2020-06-30-webscraping-with-rselenium-and-rvest/](https://ivanmillanes.netlify.app/post/2020-06-30-webscraping-with-rselenium-and-rvest/){.external target="_blank"}


# Thanks!

<br>
<br>

More complete presentation (100+ slides):

[https://www.rselenium-teaching.etiennebacher.com](https://www.rselenium-teaching.etiennebacher.com){.external target="_blank"}:

* `RSelenium` installation issues
* error handling
* more details in general






# Appendix {.appendix}

## Appendix {#contributors-results}

For reference, here's the code to extract the list of contributors:

```{r appendix, echo=TRUE}
library(rvest)

html <- read_html("contributors.html") 

bullet_points <- html %>% 
  html_elements(css = "div.col-xs-12 > ul > li") %>% 
  html_text()

blockquote <- html %>% 
  html_elements(css = "div.col-xs-12.col-sm-7 > blockquote") %>% 
  html_text() %>% 
  strsplit(., split = ", ")

blockquote <- blockquote[[1]] %>% 
  gsub("\\r|\\n|\\.|and", "", .)

others <- html %>% 
  html_elements(xpath = "/html/body/div/div[1]/div[2]/p[5]") %>% 
  html_text() %>% 
  strsplit(., split = ", ")

others <- others[[1]] %>% 
  gsub("\\r|\\n|\\.|and", "", .)

all_contributors <- c(bullet_points, blockquote, others)
```


## Appendix 

```{r appendix, eval=TRUE, echo = FALSE}
```

```{r eval=TRUE, echo = FALSE}
all_contributors[1:136] 
```

[Back](#contributors-last-step)



## Session information 

<br>

```{r echo = FALSE, eval = TRUE}
sessioninfo::session_info()
```

